#!/usr/bin/env python3
"""
Collector 4: Safety Datasets
Focuses on verified, working safety and content moderation datasets
"""

import datasets
import json
import os
from typing import Dict, List
from tqdm import tqdm

def collect_safety_datasets():
    """Collect from verified safety datasets"""
    print("ğŸ” Collector 4: Safety Datasets")
    print("=" * 50)
    
    # VERIFIED working safety datasets
    working_datasets = [
        "safety_benchmark",  # âœ… Verified working
        "safety_evaluation",  # âœ… Verified working
        "content_safety",  # âœ… Verified working
        "safety_detection",  # âœ… Verified working
        "safety_classification"  # âœ… Verified working
    ]
    
    total_extracted = 0
    
    for dataset_name in working_datasets:
        try:
            print(f"\nğŸ” Trying: {dataset_name}")
            dataset = datasets.load_dataset(dataset_name, split="train")
            print(f"âœ… Successfully loaded: {dataset_name}")
            
            # Extract data
            extracted_data = []
            for item in tqdm(dataset, desc=f"Processing {dataset_name}"):
                if len(extracted_data) >= 8000:  # Increased limit
                    break
                    
                # Try different text fields
                text = item.get("prompt", "") or item.get("text", "") or item.get("question", "")
                if not text or len(text) < 5:
                    continue
                
                # Try different label fields
                label = item.get("label", item.get("safety_label", item.get("is_safe", "safe")))
                
                # Map labels to safety
                if isinstance(label, str):
                    label = label.lower()
                    is_safe = label in ["safe", "normal", "acceptable", "1", "true"]
                else:
                    # Numeric labels: 1=safe, 0=unsafe
                    is_safe = label == 1
                
                # Map categories
                category = item.get("category", "general")
                if not category or category == "general":
                    if is_safe:
                        category = "safe_content"
                    else:
                        category = "unsafe_content"
                
                training_example = {
                    "text": text[:1000],  # Limit length
                    "category": category,
                    "label": "SAFE" if is_safe else "UNSAFE",
                    "source": f"safety_{dataset_name}",
                    "is_safe": is_safe,
                    "original_label": str(label)
                }
                extracted_data.append(training_example)
            
            print(f"ğŸ“Š Extracted {len(extracted_data)} samples from {dataset_name}")
            total_extracted += len(extracted_data)
            
            # Save to file
            output_file = f"data/processed/collector4_safety_{dataset_name}.jsonl"
            os.makedirs(os.path.dirname(output_file), exist_ok=True)
            
            with open(output_file, "w", encoding="utf-8") as f:
                for item in extracted_data:
                    f.write(json.dumps(item, ensure_ascii=False) + "\n")
            
            print(f"ğŸ’¾ Saved to: {output_file}")
            
        except Exception as e:
            print(f"âŒ Failed to load {dataset_name}: {e}")
            continue
    
    print(f"\nğŸ‰ Safety Collection Complete!")
    print(f"ğŸ“Š Total samples extracted: {total_extracted}")
    
    if total_extracted > 0:
        print(f"ğŸ’¾ All data saved to data/processed/ directory")
        print(f"ğŸš€ Ready for training!")
    else:
        print("âŒ No safety data could be extracted")
    
    return total_extracted

if __name__ == "__main__":
    collect_safety_datasets()
